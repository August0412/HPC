Install Slurm 

1. Install and configure Munge.

Create a user for munge and slurm. As both require same uid and gid on all nodes we manually specify the gid and uid.

sudo groupadd -g 1010 munge
sudo useradd -m -c "MUNGE User" -d /var/lib/munge -u 1010 -g munge -s /sbin/nologin munge 
sudo groupadd -g 1011 slurm 
sudo useradd -m -c "SLURM workload manager" -d /var/lib/slurm -u 1011 -g slurm -s /bin/bash slurm

sudo yum install munge -y 
sudo /usr/sbin/create-munge-key
sudo scp /etc/munge/munge.key  .
sudo chown admin munge.key
scp munge.key to all nodes.
run following commands on all nodes.
sudo cp munge.key /etc/munge
sudo chown -R munge: /etc/munge/ /var/log/munge/ /var/lib/munge/ /run/munge/

sudo chmod 0700 /etc/munge/ /var/log/munge/ /var/lib/munge/ 
sudo systemctl start munge
sudo systemctl enable munge ........on all nodes
if required... sudo chmod 755 /run/munge
Test on master using 
munge -n | unmunge

 munge -n | ssh <somehost_in_cluster> unmunge
You should see following message.
STATUS:           Success (0)
ENCODE_HOST:      master (192.168.230.159)
ENCODE_TIME:      2024-11-20 20:15:17 +0530 (1732113917)
DECODE_TIME:      2024-11-20 20:15:17 +0530 (1732113917)
TTL:              300
CIPHER:           aes128 (4)
MAC:              sha256 (5)
ZIP:              none (0)
UID:              admin (1000)
GID:              admin (1000)
LENGTH:           0
The munge installation is successful.

Install Slurm - MariaDB Installation
Master/Login node
1. sudo yum install mariadb-server -y
2. sudo systemctl start mariadb
3. sudo systemctl enable MariaDB
4. mysql_secure_installation ..... provide a root password
5. sudo vi /etc/my.cnf.d/innodb.cnf
and add following
[mysqld]
 innodb_buffer_pool_size=1024M
 innodb_log_file_size=64M
 innodb_lock_wait_timeout=900

save the file.
6. sudo systemctl stop mariaDB
7. sudo mv /var/lib/mysql/ib_logfile? /tmp/
8. sudo systemctl start mariaDB

Slurm Installation - All Nodes
1. sudo dnf config-manager --set-enabled crb
2. sudo dnf install epel-release epel-next-release -y
3. sudo yum install openssl openssl-devel pam-devel rpm-build numactl numactl-devel hwloc hwloc-devel lua lua5.1-luv-devel readline-devel rrdtool-devel ncurses-devel man2html libibmad libibumad -y
4. sudo yum install autoconf automake mariadb-devel munge-devel perl perl-devel dbus dbus-devel -y
5. wget https://download.schedmd.com/slurm/slurm-24.11.0-0rc2.tar.bz2
6. sudo rpmbuild -ta slurm-24.11.0-0rc2.tar.bz2
7. su -
8. cd rpmbuild/RPMS/x86_64/
9. yum --nogpgcheck localinstall slurm-* 
10. Exit

Slurm Installation - Master/Login node

1. mysql -u root -p
2. grant all on slurm_acct_db.* TO 'slurm'@'localhost' identified by 'some_pass' with grant option;
3. create database slurm_acct_db;
4. exit
5. Create the /etc/slurm/slurmdbd.conf  and add following.
AuthType=auth/munge
DbdAddr=localhost
#DbdHost=master0
DbdHost=localhost
DbdPort=6819
SlurmUser=slurm
DebugLevel=4
LogFile=/var/log/slurm/slurmdbd.log
PidFile=/run/slurmdbd/slurmdbd.pid
StorageType=accounting_storage/mysql
StorageHost=localhost
StorageLoc=slurm_acct_db
StoragePass=some_pass #type your password
StorageUser=slurm
###Setting database purge parameters
PurgeEventAfter=12months
PurgeJobAfter=12months
PurgeResvAfter=2months
PurgeStepAfter=2months
PurgeSuspendAfter=1month
PurgeTXNAfter=12months
PurgeUsageAfter=12months

save the file.
6.sudo chown slurm:slurm /etc/slurm/slurmdbd.conf
7. sudo chmod -R 600 /etc/slurm/slurmdbd.conf
8. sudo mkdir /var/log/slurm
9. sudo touch /var/log/slurm/slurmdbd.log 
10. sudo chown -R slurm /var/log/slurm/ 
11.sudo systemctl start slurmdbd
12. sudo systemctl status slurmdb
13. sudo systemctl enable slurmdb
14. go to the following website
http://slurm.schedmd.com/configurator.easy.html 
15. sudo vi /etc/slurm/slurm.conf
and add following from above website.

# slurm.conf file generated by configurator easy.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ClusterName=Demo-hpc
SlurmctldHost=master
#
#MailProg=/bin/mail
#MpiDefault=
#MpiParams=ports=#-#
ProctrackType=proctrack/cgroup
ReturnToService=1
SlurmctldPidFile=/var/run/slurmctld.pid
#SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
#SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser=slurm
#SlurmdUser=root
StateSaveLocation=/var/spool/slurmctld
#SwitchType=
TaskPlugin=task/affinity,task/cgroup
#
#
# TIMERS
#KillWait=30
#MinJobAge=300
#SlurmctldTimeout=120
#SlurmdTimeout=300
#
#
# SCHEDULING
SchedulerType=sched/backfill
SelectType=select/cons_tres
#
#
# LOGGING AND ACCOUNTING
#AccountingStorageType=
#JobAcctGatherFrequency=30
#JobAcctGatherType=
#SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurm/slurmctld.log
#SlurmdDebug=info
SlurmdLogFile=/var/log/slurm/slurmd.log
#
#
# COMPUTE NODES
NodeName=node2 NodeAddr=192.168.230.147 CPUs=2 RealMemory=4GB Sockets=2 CoresPerSocket=1 ThreadsPerCore=1 State=UNKNOWN
PartitionName=demo1 Nodes=ALL Default=YES MaxTime=INFINITE State=UP

save the file.

16. sudo cp /etc/slurm/slurm.conf ~
17.sudo cp /etc/slurm/slurmdbd.conf ~
18. sudo chmod 666 slurmdbd.conf
19. scp slurm*.* node2:/home/admin
20. sudo mkdir /var/spool/slurmctld
21. sudo chown slurm:slurm /var/spool/slurmctld
22.sudo chmod 755 /var/spool/slurmctld
23. sudo touch /var/log/slurm/slurmctld.log
24. sudo touch /var/log/slurm/slurm_jobacct.log 
25. sudo chown -R slurm:slurm /var/log/slurm/

Now go to the nodes and perofrom following
1. sudo mkdir /var/spool/slurmd
2. sudo chown slurm: /var/spool/slurmd
3. sudo chmod 755 /var/spool/slurmd
4. sudo mkdir /var/log/slurm/
5. sudo touch /var/log/slurm/slurmd.log
6. sudo chown -R slurm:slurm /var/log/slurm/slurmd.log
7. sudo cp slurm*.* /etc/slurm/
8. sudo chown -R slurm:slurm /etc/slurm/
9. sudo chmod 600 /etc/slurm/slurmdbd.conf
10. slurmd -C
you should get following output
NodeName=node3 CPUs=2 Boards=1 SocketsPerBoard=2 CoresPerSocket=1 ThreadsPerCore=1 RealMemory=3625
UpTime=0-07:45:51

check on all nodes and master.

Start the service on all compute nodes.
sudo systemctl enable slurmd.service
sudo systemctl start slurmd.service
sudo systemctl status slurmd.service

Go to the master node and 

sudo systemctl enable slurmctld.service

sudo systemctl start slurmctld.service

sudo systemctl status slurmctld.service

sinfo -N -r -l

sudo scontrol update NodeName=node1 State=DOWN 
reason=Hung_completion

sudo scontrol update NodeName=node1 State=RESUME


sinfo -N -r -l
 